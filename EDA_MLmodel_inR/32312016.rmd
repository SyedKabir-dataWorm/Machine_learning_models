---
title: "Exploratory Data Analysis and Model Building"
author: "Syed Kabir"
date: "1/21/2022"
output:
  html_document:
    keep_md : true
    toc: true # table of content true
    toc_depth: 4  # upto three depths of headings 
    number_sections: true  ## if you want number sections at each table header
    theme: united  # many options for theme, this one is my favorite.
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# installing all required packages
if( !require(plyr) ){install.packages('plyr', repos="http://cran.rstudio.com/")}
if( !require(ggplot2) ){install.packages('ggplot2', repos="http://cran.rstudio.com/")}
if( !require(psych) ){install.packages('psych')}
if( !require(gridExtra) ){install.packages('gridExtra')}
if( !require(ISLR) ){install.packages('ISLR')}
if( !require(MASS) ){install.packages('MASS')}
if( !require(caret) ){install.packages('caret', repos="http://cran.rstudio.com/")}
if( !require(car) ){install.packages('car')}
if( !require(psych) ){install.packages('psych')}
```


```{r}
library(psych)
library(gridExtra) 
library(ggplot2)
library(car)
library(caret)
library(tidyverse) 
library(magrittr)
library(MASS)
library(psych)
library(class)

```

# Introduction:
This assignment is to perform exploratory data analysis on insurance data followed by appropriate modelling to predict whether a customer of a insurance company will stay or leave. Sixteen distinguishing factors are given in the data-set in understanding the customer churn. Two data-sets are given:

  a) trainSet.csv which contains 27126 observations

  b) testSet.csv which contains 6782 observations

  So, trainSet data will be used to perform exploratory analysis and training the model. Two distinguished models will be build-up using training data. On the other hand, testSet.csv will be used for identifying model accuracy and selecting better model.

```{r}
insurance1 <- read.csv('trainSet.csv', header = TRUE, sep = ',')
testset <- read.csv('testSet.csv', header = TRUE, sep = ',')
```

```{r}
head(insurance1)
```
```{r}
# Now, let's check the number of variables, rows, variable names and their types
str(insurance1)
```
The structure of the training data set shows that:

feature_o, feature_1, feature_2, feature_3,feature_4, feature_5 and feature_6 are continuous type numerical variables.

feature_7, feature_8, feature_9, feature_10,feature_11, feature_12, feature_13, feature_14 and feature_15 are discrete type variables.

```{r}
# Let's check how many unique values are avilable for each variable:
sapply(insurance1, function(x) length(unique(x)))
```
So, feature_1 has highest highest number of unique values.
feature_10, feature_11, feature_12, labels are of binary.
feature_8 and feature_13 have three unique values : 0,1 and 2
feature_9 and feature_15 have four unique values : 0,1,2 and 3

# EDA:

```{r}
summary(insurance1)
```
```{r}
round(describe(insurance1), 3)
```
Some observations from the Summary:

* The range for feature_1,feature_3,feature_4 and feature_6 is relatively high (from 19 to 29) across the samples compared to other variables.
* The range for feature_0,feature_5,feature_11 is moderately large (from 7 to 11) across the samples compared to other variables. The value of other features varies from 0 to 3.157
* The minimum value for feature_1 to feature_6 is negative. For others, minimum value is zero.
* The mean value for feature_1 to feature_6 is very close to zero(some are negatives). For others, mean is positive.
* The median value for feature_1 to feature_6 is negatives. For others, median value is positive.
* So values of different features varie in different scales. If the data isnâ€™t normalized it will lead to a baised outcome.

## Summary Statistics and Variances: After data normalization 

```{r}
#Normalization
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x))) }
```

```{r}
insurance <- as.data.frame(lapply(insurance1, normalize))
summary(insurance)
```
 
 So, all the features are normalised in the range of 0 to 1. Binary variables remain unchanged. The variance of the data after normalization is:
 
```{r}
sort(apply(insurance[-17], 2, sd))
```
 
So, after normaloisation, it is clear that feature_1,feature_3,feature_4,feature_6 has very low variance and feature_11 has the highest variance.

## Univariate Analysis:

### Data is imbalanced in Target Variable:

```{r}
p16<-ggplot(aes(x=labels), data =  insurance) +
   geom_histogram(color = I('black'), fill = "blue") +
   ggtitle('labels distribution')
p16
```


Majority of the labels are zero. So, the data is heavily imbalanced

### Outliers among the features:

* Boxplot of feature_0, feature_1, feature_3, feature_4, feature_5, feature_6 are showing many outliers for higher values.
* Only feature_1 and feature_15 have outliers for lower values.

```{r}
par(mfrow = c(3,6)) # 6 x 2 grid
for (i in 1:(length(insurance))) {
        boxplot(insurance[,i], main = names(insurance[i]), type="l", col = 'lightblue') 
}
```

It is very hard to assess the distribution observing the boxplot for many features. So let's have a look on distribution for features

### Histogram of all variables:

```{r}
options(repr.plot.width=15, repr.plot.height=15)
p0<-ggplot(aes(x=feature_0), data =  insurance) +
    geom_histogram(color = I('black'), fill = "red") + 
    ggtitle('feature_0 distribution')

p1<-ggplot(aes(x=feature_1), data =  insurance) +
    geom_histogram(color = I('black'), fill = "red") + 
    ggtitle('feature_1 distribution')

p2<-ggplot(aes(x=feature_2), data =  insurance) +
    geom_histogram(color = I('black'), fill = "red") + 
    ggtitle('feature_2 distribution')

p3<-ggplot(aes(x=feature_3), data =  insurance) +
    geom_histogram(color = I('black'), fill = "red") + 
    ggtitle('feature_3 distribution')

p4<-ggplot(aes(x=feature_4), data =  insurance) +
    geom_histogram(color = I('black'), fill = "red") + 
    ggtitle('feature_4 distribution')

p5<-ggplot(aes(x=feature_5), data =  insurance) +
    geom_histogram(color = I('black'), fill = "red") + 
    ggtitle('feature_5 distribution')

p6<-ggplot(aes(x=feature_6), data =  insurance) +
    geom_histogram(color = I('black'), fill = "red") +
    ggtitle('feature_6 distribution')

p7<-ggplot(aes(x=feature_7), data =  insurance) +
    geom_histogram(color = I('black'), fill = "red") +
    ggtitle('feature_7 distribution')

p8<-ggplot(aes(x=feature_8), data =  insurance) +
    geom_histogram(color = I('black'), fill = "red") +
    ggtitle('feature_8 distribution')

p9<-ggplot(aes(x=feature_9), data =  insurance) +
    geom_histogram(color = I('black'), fill = "red") +
    ggtitle('feature_8 distribution')

p10<-ggplot(aes(x=feature_10), data =  insurance) +
    geom_histogram(color = I('black'), fill = "red") +
    ggtitle('feature_10 distribution')

p11<-ggplot(aes(x=feature_11), data =  insurance) +
    geom_histogram(color = I('black'), fill = "red") +
    ggtitle('feature_11 distribution')

p12<-ggplot(aes(x=feature_12), data =  insurance) +
    geom_histogram(color = I('black'), fill = "red") +
    ggtitle('feature_12 distribution')

p13<-ggplot(aes(x=feature_13), data =  insurance) +
    geom_histogram(color = I('black'), fill = "red") +
    ggtitle('feature_13 distribution')

p14<-ggplot(aes(x=feature_14), data =  insurance) +
    geom_histogram(color = I('black'), fill = "red") +
    ggtitle('feature_14 distribution')

p15<-ggplot(aes(x=feature_15), data =  insurance) +
    geom_histogram(color = I('black'), fill = "red") +
    ggtitle('feature_15 distribution')

#p16<-ggplot(aes(x=labels), data =  insurance) +
#    geom_histogram(color = I('black'), fill = "red") +
#    ggtitle('labels distribution')

# plot all 16, 4 x 4 

grid.arrange(p0, p1, p2, p3, p4, p5, p6, p7, p8, p9, p10, p11, p12, p13, p14, p15, ncol = 4)
```

* feature_0 is showing close to a uniform distribution with right skewness.
* feature_1, feature_3, feature_4,feature_5 and feature_6 are heavily right skewed.
* feature_2 distribution is multimodal. Significant number of values are negative.
* labels distribution clearly shows that only a fraction of customer will churn.

## Bivariate Analysis:

### Correlations among the features:

```{r}
round(cor(insurance[1:17]),3)
```
The following features are highly corelated:
* feature 5 and feature 15 are highly corelated negatively.

The following features are moderatelyly corelated:
* feature 0 and feature 8 negatively
* feature 5 and feature 6 positively
* feature 6 and feature 15 negatively
* feature 13 and feature 14 positively

The following features are low corelated:
* feature 0 and feature 11 negatively
* feature 2 and feature 4 positively
* feature 5 and feature 13 negatively
* feature 6 and feature 13 negatively
* feature 7 and feature 9 positively
* feature 11 and feature 13 positively
* feature 11 and feature 14 positively
* feature 13 and feature 15 positively

The following features are corelated with labels:
* feature_3 positively moderate correlation
* feature_5, feature_6 low positive correlation
* feature_11, feature_13 low negative correlation

### Plot for Strongest Corelations:

```{r}
plot(insurance$feature_5, insurance$feature_15)
```

* Very high value (=3) of feature_15 is only available when feature_5 values are relatively low(<6)
* feature_5 values are in higher range more when feature_15 values are relatively low.

### Investigating feature_3 which has strongest correlations with labels
```{r}
# For better analysis, labels is converted to factor type
# As feature_3 has higher correlation with labels, let's investigate the this two variables
insurance$labels <- as.factor(insurance$labels)
p4 = ggplot(aes(x=feature_3),data =insurance) + geom_density(aes(fill = labels)) +  ggtitle('feature_3 density by labels')
p4
```

It is clearly evident that data density is distributed with low spreading for feature_3 when labels value is 0 compared to 1 value.

### Investigating data spreading of all features w.r.t the classes of labels

```{r}
options(repr.plot.width=15, repr.plot.height=15)
p1=ggplot(aes(x=feature_0),data =insurance) + geom_density(aes(fill = labels)) +  ggtitle('feature_0 density by labels') + theme(plot.title = element_text(size = 5, face = "bold"),axis.text=element_text(size=5), axis.title=element_text(size=5),legend.title = element_text(size = 5), 
               legend.text = element_text(size = 5)) 
p2= ggplot(aes(x=feature_1),data =insurance) + geom_density(aes(fill = labels)) +  ggtitle('feature_1 density by labels') + theme(plot.title = element_text(size = 5, face = "bold"),axis.text=element_text(size=5), axis.title=element_text(size=5),legend.title = element_text(size = 5), 
               legend.text = element_text(size = 5)) 
p3 = ggplot(aes(x=feature_2),data =insurance) + geom_density(aes(fill = labels)) +  ggtitle('feature_2 density by labels') + theme(plot.title = element_text(size = 5, face = "bold"),axis.text=element_text(size=5), axis.title=element_text(size=5),legend.title = element_text(size = 5), 
               legend.text = element_text(size = 5)) 
p4 = ggplot(aes(x=feature_3),data =insurance) + geom_density(aes(fill = labels)) +  ggtitle('feature_3 density by labels') + theme(plot.title = element_text(size = 5, face = "bold"),axis.text=element_text(size=5), axis.title=element_text(size=5),legend.title = element_text(size = 5), 
               legend.text = element_text(size = 5)) 
p5 = ggplot(aes(x=feature_4),data =insurance) + geom_density(aes(fill = labels)) +  ggtitle('feature_4 density by labels') + theme(plot.title = element_text(size = 5, face = "bold"),axis.text=element_text(size=5), axis.title=element_text(size=5),legend.title = element_text(size = 5), 
               legend.text = element_text(size = 5)) 
p6 = ggplot(aes(x=feature_5),data =insurance) + geom_density(aes(fill = labels)) +  ggtitle('feature_5 density by labels') + theme(plot.title = element_text(size = 5, face = "bold"),axis.text=element_text(size=5), axis.title=element_text(size=5),legend.title = element_text(size = 5), 
               legend.text = element_text(size = 5)) 
p7 = ggplot(aes(x=feature_6),data =insurance) + geom_density(aes(fill = labels)) +  ggtitle('feature_6 density by labels') + theme(plot.title = element_text(size = 5, face = "bold"),axis.text=element_text(size=5), axis.title=element_text(size=5),legend.title = element_text(size = 5), 
               legend.text = element_text(size = 5)) 
p8 = ggplot(aes(x=feature_7),data =insurance) + geom_density(aes(fill = labels)) +  ggtitle('feature_7 density by labels') + theme(plot.title = element_text(size = 5, face = "bold"),axis.text=element_text(size=5), axis.title=element_text(size=5),legend.title = element_text(size = 5), 
               legend.text = element_text(size = 5)) 
p9 = ggplot(aes(x=feature_8),data =insurance) + geom_density(aes(fill = labels)) +  ggtitle('feature_8 density by labels') + theme(plot.title = element_text(size = 5, face = "bold"),axis.text=element_text(size=5), axis.title=element_text(size=5),legend.title = element_text(size = 5), 
               legend.text = element_text(size = 5)) 
p10 = ggplot(aes(x=feature_9),data =insurance) + geom_density(aes(fill = labels)) +  ggtitle('feature_9 density by labels') + theme(plot.title = element_text(size = 5, face = "bold"),axis.text=element_text(size=5), axis.title=element_text(size=5),legend.title = element_text(size = 5), 
               legend.text = element_text(size = 5)) 
p11 = ggplot(aes(x=feature_10),data =insurance) + geom_density(aes(fill = labels)) +  ggtitle('feature_10 density by labels') + theme(plot.title = element_text(size = 5, face = "bold"),axis.text=element_text(size=5), axis.title=element_text(size=5),legend.title = element_text(size = 5), 
               legend.text = element_text(size = 5)) 
p12 = ggplot(aes(x=feature_11),data =insurance) + geom_density(aes(fill = labels)) +  ggtitle('feature_11 density by labels') + theme(plot.title = element_text(size = 5, face = "bold"),axis.text=element_text(size=5), axis.title=element_text(size=5),legend.title = element_text(size = 5), 
               legend.text = element_text(size = 5)) 
p13 = ggplot(aes(x=feature_12),data =insurance) + geom_density(aes(fill = labels)) +  ggtitle('feature_12 density by labels') + theme(plot.title = element_text(size = 5, face = "bold"),axis.text=element_text(size=5), axis.title=element_text(size=5),legend.title = element_text(size = 5), 
               legend.text = element_text(size = 5)) 
p14 = ggplot(aes(x=feature_13),data =insurance) + geom_density(aes(fill = labels)) +  ggtitle('feature_13 density by labels') + theme(plot.title = element_text(size = 5, face = "bold"),axis.text=element_text(size=5), axis.title=element_text(size=5),legend.title = element_text(size = 5), 
               legend.text = element_text(size = 5)) 
p15 = ggplot(aes(x=feature_14),data =insurance) + geom_density(aes(fill = labels)) +  ggtitle('feature_14 density by labels') + theme(plot.title = element_text(size = 5, face = "bold"),axis.text=element_text(size=5), axis.title=element_text(size=5),legend.title = element_text(size = 5), 
               legend.text = element_text(size = 5)) 
p16 = ggplot(aes(x=feature_15),data =insurance) + geom_density(aes(fill = labels)) +  ggtitle('feature_15 density by labels') + theme(plot.title = element_text(size = 5, face = "bold"),axis.text=element_text(size=5), axis.title=element_text(size=5),legend.title = element_text(size = 5), 
               legend.text = element_text(size = 5)) 

grid.arrange( p1,p2,p3,p4,p5,p6,p7,p8,p9,p10,p11,p12,p13,p14,p15,p16, ncol = 3)
```

No feature shows specific independence in data spreading for each values of labels.

### Investigating relations among continuous features:

```{r}
# Now let's investigate the continuous features and labels in the data.
library(car) 
scatterplotMatrix(insurance[,c(1:7)],cex=0.2,main="Scatterplot Matrix for Insurance Data Using Floating Variables")
```

* Feature_3 has nonlinear negative relationship with almost all numeric features.
* Feature_1 and feature_4 has non-linear negative relationship with feature_6
* feature_1 and feature_4 relationship is also nonlinear negative.

## Multivariate EDA :

### Feature_3 data spreading for each labels and for each value of feature_15:

```{r}
# Feature_3 is highlighted here as it has better relationship with labels
ggplot(aes(x=feature_3),data =insurance) + geom_density(aes(fill = labels)) +
    facet_wrap(~feature_15) +
    ggtitle('Density of feature_3 with respect to feature_15 and labels')
```

* From the above plot, there doesn't seem to be any specific seprability of labels considering data for feature_3 and feature_15. 
* However value 0 of labels tends to exhibit a higher feature_3 value density than value 1 of labels for all the values of feature_15. 
* Value 1 of labels tends to exhibit a spreader distribution of feature_3 value than value 1 of labels for all the values of feature_15.
* The feature_15 value of .667 exhibits a narrower spread for feature_3 value between 0 and 0.25 compared to other feature_15 values.

### Feature_3 data spreading for each labels and for each value of feature_13:

```{r}

ggplot(aes(x=feature_3),data =insurance) + geom_density(aes(fill = labels)) +
    facet_wrap(~feature_13) +
    ggtitle('Density of feature_3 with respect to feature_13 and labels')
```

* From the above analysis plot,feature_3 shows more independence for each labels for higher value (when value is 1) of feature_13.
* However value 0 of labels tends to exhibit a higher feature_3 value density than value 1 of labels for all the values of feature_13. 
* Value 1 of labels tends to exhibit a spreader distribution of feature_3 value than value 1 of labels for all the values of feature_13.

### feature_3 values relationship with discrete variables by labels:

```{r}
t1=ggplot(aes(x = feature_14, y = feature_3), data = insurance) + 
    geom_point(aes(color=labels),alpha=1/4, position = 'jitter') +
    ggtitle(' Feature_3 and  feature_14 Relationship by labels')
t2= ggplot(aes(x = feature_15, y = feature_3), data = insurance) + 
    geom_point(aes(color=labels),alpha=1/4, position = 'jitter') +
    ggtitle(' Feature_3 and  feature_15 Relationship by labels')
t3 = ggplot(aes(x = feature_13, y = feature_3), data = insurance) + 
    geom_point(aes(color=labels),alpha=1/4, position = 'jitter') +
    ggtitle(' Feature_3 and  feature_13 Relationship by labels')
t4= ggplot(aes(x = feature_12, y = feature_3), data = insurance) + 
    geom_point(aes(color=labels),alpha=1/4, position = 'jitter') +
    ggtitle(' Feature_3 and  feature_12 Relationship by labels')
t5= ggplot(aes(x = feature_11, y = feature_3), data = insurance) + 
    geom_point(aes(color=labels),alpha=1/4, position = 'jitter') +
    ggtitle(' Feature_3 and  feature_11 Relationship by labels')

grid.arrange( t1,t2,t3,t4,t5, ncol = 1)

```

```{r}
t6 =ggplot(aes(x = feature_10, y = feature_3), data = insurance) + 
    geom_point(aes(color=labels),alpha=1/4, position = 'jitter') +
    ggtitle(' Feature_3 and  feature_10 Relationship by labels')
t7= ggplot(aes(x = feature_9, y = feature_3), data = insurance) + 
    geom_point(aes(color=labels),alpha=1/4, position = 'jitter') +
    ggtitle(' Feature_3 and  feature_9 Relationship by labels')
t8= ggplot(aes(x = feature_8, y = feature_3), data = insurance) + 
    geom_point(aes(color=labels),alpha=1/4, position = 'jitter') +
    ggtitle(' Feature_3 and  feature_8 Relationship by labels')
t9= ggplot(aes(x = feature_7, y = feature_3), data = insurance) + 
    geom_point(aes(color=labels),alpha=1/4, position = 'jitter') +
    ggtitle(' Feature_3 and  feature_7 Relationship by labels')

grid.arrange( t6,t7,t8,t9, ncol = 1)
```

All the plots indicate that for the same values, label 1 has higher feature_3 than the label 0 on average across all the feature values.

### Feature_3 and feature_5 relationship for each value of feature_11 by labels 
```{r}
ggplot(aes(x=feature_3, y=feature_5),data = insurance) + 
    geom_jitter(aes(color = labels, bg = labels), alpha=1/10,,pch=21, cex=4) +
    facet_wrap(~feature_11) +
    scale_color_brewer(type = 'div') +
    ggtitle('Feature_5 and feature_3 relationship for each classes of feature_11')
```

* feature_3 value is higher for feature_5 when label is 1 for any value of feature_11.
* For label 0, feature_5 values are in higher ranges when feature_11 value is 1 

## Summary:

* The dataset is heavily biased to the value of 0 of labels.
* Feature_13 and feature_11 varies relatively more as they possess higher standard deviation.
* Only feature_5 and feature_15 has strong correlations.
* Feature_3 has the highest correlation with labels which is a moderate and positive correlation.
* Labels are weakly correlated to feature_5, feature_6, feature_11, feature_13 and feature_15.
* Data are not quite separatable by each labels for all variables.
* In case of multivariate analysis, feature_3 shows more independence among each labels only for higher value of feature_13.  
* For records with the value of 1 for labels, the feature_3 value is higher than that of labels with 0 across all the values of feature_14 and feature_15

### Key decisions obtained from EDA:

* We have observed from the graphs and statistics of exploratory data analysis that the classes (labels) are not quite separate across the data for almost all the variables. 
* So a simple linear regression will not be good enough to be an efficient classifier.
* As data set is quite large and normality in the data does not hold and classes are not easily separable, Linear Discriminant Analysis (LDA) is not suitable for this data.
* Here, logistic regression and KNN has been chosen as the classifier for building the model. 


# Model Development:

## Multiple Logistic Regression Classification Model:

```{r}
# Creating training and validation data sets
sample_size = floor(0.8*nrow(insurance))
set.seed(777)
# randomly split data in r
splitted = sample(seq_len(nrow(insurance)),size = sample_size)
train =insurance[splitted,]
valid =insurance[-splitted,]
labels_valid=valid$labels
```

### First Logistic Model: taking all features into account:

```{r}
fit.glm_1 = glm(labels ~ ., data = train, family = binomial)
summary(fit.glm_1)

```
```{r}
# Predicting model accuracy on validation data set followed by generating confusion matrix 
probs <- predict(fit.glm_1, valid, type = "response")
pred.glm_1 <- rep(0, length(probs))
pred.glm_1[probs >0.5] <- 1
confusionMatrix(table(pred.glm_1, labels_valid), labels = 1)
```

So first logistic model accuracy on validation data set is 0.880


### Second Logistic model: with feature-selection using anova:

```{r}
# Selecting important features using anova
anova(fit.glm_1, test = "Chisq")
```

So, feature_0, feature_1, feature_2, feature_10,feature_14 can be removed from the model as the drop of deviance is relatively small (<35).

```{r}
fit.glm_2 = glm(labels ~ .-feature_0-feature_1-feature_2-feature_10-feature_14, data = train, family = binomial)

summary(fit.glm_2)
```

```{r}
probs <- predict(fit.glm_2, valid, type = "response")
pred.glm_2 <- rep(0, length(probs))
pred.glm_2[probs >0.5] <- 1
confusionMatrix(table(pred.glm_2, labels_valid), labels = 1)
```


* So, model accuracy remains same (0.880) and precision decreased slightly. However, model complexity is reduced significantly.

### Third Logistic model: Further feature reduction based on Z-static:

* We know that a bigger Z static(both positive and negative) indicates that the corresponding true regression coefficient is not 0 and the corresponding X-variable matters. 
* So, here, we can remove the feature_7 and feature_8 from the second model as they showed relatively small z value(-5>Z>5) (see output at line 481: summary statistics of second model )
* In that case, only feature_0,feature_1,feature_2,feature_7,feature_8,feature_10,feature_14 shall remain in the third logistic model

```{r}
fit.glm_3 = glm(labels ~ .-feature_0-feature_1-feature_2-feature_7-feature_8-feature_10-feature_14, data = train, family = binomial)
summary(fit.glm_3)

```
```{r}
probs <- predict(fit.glm_3, valid, type = "response")
pred.glm_3 <- rep(0, length(probs))
pred.glm_3[probs >0.5] <- 1
confusionMatrix(table(pred.glm_3, labels_valid), labels = 1)
```
```{r}
with(fit.glm_3, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE))
```

* So, model performance did not change at all. The number of model predictors are decreased. The above R-chunk proves that third logistic model varies significantly than null model.

### Fourth Multiple Logistic Model: Feature selected from EDA:

From EDA, we see that feature_3,feature_5,feature_6,feature_11, feature_13, feature_15 have better correlations with labels.

```{r}
set.seed(3)
fit.glm_4 = glm(labels ~ feature_3 +feature_5+feature_6+feature_11+feature_13+ feature_15 , data = train, family = binomial)
summary(fit.glm_4)
```

```{r}
probs <- predict(fit.glm_4, valid, type = "response")
pred.glm_4 <- rep(0, length(probs))
pred.glm_4[probs >0.5] <- 1
confusionMatrix(table(pred.glm_4, labels_valid), labels = 1)
```


* Though the model accuracy remains almost same,precision is reduced significantly.

* Considering the above 4 logistic models, model-3 is preferable as it has less model complexity (9 predictors) and 

* Model accuracy and precision is almost same to that of best accuracy shown model,i.e model-1. Following table confirms that:
 
|Model_Name                  | Evaluation_Technique     | Accuracy  | Model Complexity      | Specificity |
|----------------------------|--------------------------|---------- |-----------------------|-------------|
|Logistic Regression-model-1 | Validation set approach  | 0.8802    | Most (All features)   | 0.1911      |
|Logistic Regression-model-2 | Validation set approach  | 0.8800    | Moderate (11 features)| 0.1782      |
|Logistic Regression-model-3 | Validation set approach  | 0.8800    | Moderate (9 features) | 0.1811      |
|Logistic Regression-model-4 | Validation set approach  | 0.8806    | Moderate (6 features) | 0.1679      |



### Cross-checking Selected logistic model performance on whole training set:

```{r}
insurance.labels<-insurance$labels
probs <- predict(fit.glm_3, insurance, type = "response")
pred.glm_final <- rep(0, length(probs))
pred.glm_final[probs >0.5] <- 1
confusionMatrix(table(pred.glm_final, insurance.labels), labels = 1)
```

* So model accuracy is 0.889. That means, chosen logistic model performance is quite consistent among validation and whole training data set. Now let's check another classifier for having better model.


## K-Nearest Neighbour(KNN) Classification:

### First KNN model: considering all features of the data-set:

```{r}
library(class)
```


```{r}
set.seed (1)
knn.pred1 <- knn(train[,-17], valid[,-17], train$labels , k = 3)
confusionMatrix(table(knn.pred1, valid$labels), labels = 1)
```

So, KNN model-1 accuracy is 0.878

### Second KNN Model: Considering only the chosen features of ultimately selected logistic model:

```{r}
train.x <- train[,c("feature_3","feature_4","feature_5","feature_6","feature_9","feature_11","feature_12","feature_13","feature_15")]
valid.x <- valid[,c("feature_3","feature_4","feature_5","feature_6","feature_9","feature_11","feature_12","feature_13","feature_15")]
```

```{r}
set.seed (1)
knn.pred2 <- knn(train.x, valid.x, train$labels , k = 3)
confusionMatrix(table(knn.pred2, valid$labels), labels = 1)
```

So, KNN model-2 accuracy : 0.874

Now selecting appropriate K hyper-parameter:

```{r}
i=1
k.optm=1
for (i in 1:20){
 knn.mod <- knn(train.x,valid.x,train$labels, k=i)
 k.optm[i] <- 100 * sum(valid$labels == knn.mod)/NROW(valid$labels)
 k=i
 cat('for K ',k,':',k.optm[i],';')
}
```

* In this case, we can select K=10.
* Comparing both models of KNN, based on validation set method, model1 performs better than model2.
* p-value of both KNN model is also very low which justifies the statistical significance of the models.
* However, the variation in model accuracy might be the result of how the data is splitted. So, use of another validation technique for cross checking is important. 
* For cross-checking model-performance, K-fold cross validation technique can be used which will nullify the impact of data-splitting which is available in validation-set technique.

### Cross-checking KNN model performance using K-fold cross validation technique: selection of ultimate KNN model

#### K-Fold cross validation on KNN Model-1

```{r}
library(caret)
library(class)
folds = createFolds(insurance$labels, k = 10)
cv = lapply(folds, function(x) {
  training_fold = insurance[-x,]
  test_fold = insurance[x, ]
  y_pred = knn( training_fold[,-17], test_fold[,-17], training_fold[,17] , k = 10)
  #classifier = KNN(formula = Purchased ~ .,data = training_fold,type = 'C-classification', kernel = 'radial')
  #y_pred = predict(classifier, newdata = test_fold[-17])
  cm = table(test_fold[, 17], y_pred)
  accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
  return(accuracy)
})
accuracy = mean(as.numeric(cv))
accuracy

```


#### K-Fold cross validation on KNN Model-2:

```{r}
library(caret)
library(class)
folds = createFolds(insurance$labels, k = 10)
cv = lapply(folds, function(x) {
  training_fold = insurance[-x,c("feature_3","feature_4","feature_5","feature_6","feature_9","feature_11","feature_12","feature_13","feature_15","labels") ]
  test_fold = insurance[x,c("feature_3","feature_4","feature_5","feature_6","feature_9","feature_11","feature_12","feature_13","feature_15","labels") ]
  y_pred = knn( training_fold[,-10], test_fold[,-10], training_fold[,10] , k = 10)
  #classifier = KNN(formula = Purchased ~ .,data = training_fold,type = 'C-classification', kernel = 'radial')
  #y_pred = predict(classifier, newdata = test_fold[-17])
  cm = table(test_fold[, 10], y_pred)
  accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
  return(accuracy)
})
accuracy = mean(as.numeric(cv))
accuracy
```
* So, K-fold cross-validation ensures better accuracy of KNN model-2. And this result is more reliable compared to validation data set technique.

Now, it is time to compare all KNN model accuracy:

|Model_Name         | Evaluation_Technique              |Accuracy  | Model Complexity     |
|-------------------|-----------------------------------|----------|----------------------|
|KNN-model-1        | Validation set approach           | 0.8778   | Most (All features)  |
|KNN-model-2        | Validation set approach           | 0.8736   | Moderate (9 features)|
|KNN-model-1        | K-fold cross validation approach  | 0.8878   | Most (All features)  |
|KNN-model-2        | K-fold cross validation approach  | 0.8943   | Moderate (9 features)|

So, selected KNN model is KNN model-2 as it shows higher accuracy for K-fold cross validation approach.

### Crosschecking KNN model-2 performance using whole training data set (here it is 'insurance' data)

```{r}
set.seed (1)
knn.pred0 <- knn(train.x, insurance[,c("feature_3","feature_4","feature_5","feature_6","feature_9","feature_11","feature_12","feature_13","feature_15")], train$labels , k = 10)
confusionMatrix(table(knn.pred0, insurance$labels), labels = 1)
```
So, the validation of KNN model on whole training set provides better accuracy as it is expected; because maximum fraction of whole insurance data were used for training the model. 


# Model Evaluation Based on Test Data:

## Logistic Model Evaluation using test data  


```{r}
probs <- predict(fit.glm_3, testset, type = "response")
pred.glm_test <- rep(0, length(probs))
pred.glm_test[probs >0.5] <- 1
confusionMatrix(table(pred.glm_test, testset$labels), labels = 1)
```

* So accuracy of the model is increased from 0.88(train data) to 0.884 (test data) after evaluating model performance with test data.
* Model p-value is very low.
* However, precision is unexpectedly very high for test data set. It is increased from .18 (train) to .83 (test) 

## KNN Model Evaluation using test-data:

```{r}
set.seed (1)
#knn.test <- knn(train, testset, train$labels , k = 10)
knn.test <- knn(train[,-17], testset[,-17], train[ ,17] , k = 10)
confusionMatrix(table(knn.test, testset$labels), labels = 1)

```

* Accuracy of logistic model is 0.843 which is quite lower than it's validation on training data.
* Most importantly, it performs poor in case of prediction positive churn (0.1635) of the customer.



## Comparing Model runtime: 

Let's check computational complexity in terms of model run-time for both the logistic and KNN model


```{r}
#install.packages('tictoc')
library(tictoc)
tic("time for regression model fitting: ")
fit.glm_3 = glm(labels ~ .-feature_0-feature_1-feature_2-feature_7-feature_8-feature_10-feature_14, data = train, family = binomial)

probs <- predict(fit.glm_3, testset, type = "response")
pred.glm_test <- rep(0, length(probs))
pred.glm_test[probs >0.5] <- 1
toc()
```


```{r}
tic("time for KNN model fitting: ")
knn.test <- knn(train[-17], testset[-17], train$labels , k = 5)
toc()
```

* It is clear that KNN model took (1.64/0.15) ~ 11 times more time to run the model comparing with logistic model.
* So for very large data-set logistic model is an automatic choice as the KNN-model runs very slow.


## Comparing memory complexity:
```{r}
# #install.packages('GuessCompx')
# #install.packages("Rtools")
# update.packages('Rcpp')
# library(Rcpp)
# library(GuessCompx)
# CompEst(testset[,c(4,5,6,7,10,12,13,14,16,17)], glm, random.sampling=FALSE, max.time=10, start.size=NULL, replicates=2, strata=NULL, power.factor=2, alpha.value=0.005, plot.result=TRUE)

```


```{r}
memory_breakdown.fn <- function(model){
  
  MB <- sapply(model, FUN = function(t){ 
    
    10^(-6) * as.numeric(gsub(pattern = "bytes", replacement = "", x = utils::object.size(t))) 
    
  }, simplify = TRUE)
  return(MB)
  
}
```


### Memory Consumption of Logistic Model:

```{r}
sum(memory_breakdown.fn(model = fit.glm_3))
```

### Memory Consumption of KNN model

```{r}
sum(memory_breakdown.fn(model = knn.test))
```

## Overall evaluation of Models:


|Model_Name     | Evaluation_Technique on Test Data |Accuracy  | Model Complexity     |Specificity|Run-time |Memory  |
|---------------|-----------------------------------|----------|----------------------|-----------|---------|--------|
|Logistic-model | Validation set approach           | 0.8843   | Moderate (9 features)|0.8318     |0.15 sec |19.15 MB|
|KNN-model      | Validation set approach           | 0.8428   | Moderate (9 features)|0.1656     |1.64 sec |03.85 MB|

So, Logistic model out performs KNN model in terms of accuracy and predicting positive churn of the customer. Though it's memory consumption 
is relatively high, it is very fast compared to KNN model.

So ultimate selected model is Logistic Model.


# Model Interpretation and Inference:

At first, let's interpret the summary statistics of the ultimately selected model: it is third logistic model

```{r}
summary(fit.glm_3)
```


* The big **z-value** of feature_3 (i.e., either too positive or too negative) indicates that the corresponding true regression coefficient is not 0 and feature_3 really matters in the model. Similarly, importance of feature_15 is lowest as it has lowest absolute Z value.
* **Null deviance** shows how well the response variable is predicted by a model that includes only the intercept. For this particular model Null deviance  is 15518
* **Residual deviance** indicates how well the response variable is predicted with the implemented model.
* **AIC** provides a method for assessing the quality of the selected model through comparison of related models. In this case, the logistic model-1, 2,4 are the related models. We see that AIC for model-1 is 11504. For model-3 , AIC is 11551. But the change is not significant.

Now, let us interprete the model using confusion matrix.

```{r}
confusionMatrix(table(pred.glm_test, testset$labels), labels = 1)
```


* NIR(No information rate):  the largest proportion of the observed classes. here it is 0.88. It indicates, how much the target data are imbalanced.
* P-Value [Acc > NIR]: A hypothesis test that computes whether the overall accuracy rate is greater than the rate of the largest class. As the p-value is very high in this case, the overall accuracy rate is lower than the rate of the largest class.
* Sensitivity: the percentage of customers that did not churn are correctly identified. In this KNN model, it is 70.77% 
* Specificity: the percentage of customers that churn are correctly identified, it is 83.18%

The impact of variance has been outperformed by the correlations of the variables. For example: feature_3 standard deviation (.05) is very low, however the z-value for the feature_3 is the highest in the model. The reason is feature_3 has the highest correlations with the target variable (labels). Another reason is high-variance-features have better correlations with other variables as well which reduces the variance effect in the model.

# Variable Importance Measurent:

At first, let's check the near zero variance predictors, which have the following two characteristics
* 1. They have very few unique values relative to the number of samples
* 2. the ratio of the frequency of the most common value to the frequency of the second most common value is large
    
This kind of predictor is not only non-informative, it can break some models that we may want to fit to our data. 

the function nearZeroVar from the caret is used to idenfity those near zero variance predictors.

```{r}
nzv <- nearZeroVar(insurance, saveMetrics = TRUE)
nzv
```


So, only feature_5 variable is showing near zero variance.
* From EDA, we see that the following features have better correlations with the labels: feature_3,feature_5, feature_6, feature_11, feature_13 and feature_15
* While building logistic regression, we see that combination of few features in the model produced similar accuracy to the model which counted all features. The more important features in the model were: feature_3,feature_4,feature_5,feature_6,feature_9,feature_11,feature_12,feature_13, feature_15
* So all the important features sorted out from EDA is a subset of the features for logistic regression. we get 9 out of 16 features on which we can do further investigation.
* Here, shapleyvalue library is used to measure the importance of the variables in the data.

```{r}
#install.packages('ShapleyValue')
library(ShapleyValue)
```

```{r}
y <- insurance$labels
x <- as.data.frame(insurance[,c(4:7,10,12:14,16)])
head(x)
```

These are the important variables identified from our earlier analysis. Let's find out the shapley value for those features.

```{r}
value <- shapleyvalue(y,x)
value
```

* We see that feature_9, feature_12 and feature_15 has the lowest shapley value and feature_3 has the highest values. Feature_3 also has the strongest correlations with labels as we have seen from the EDA. 
* So feature_9, feature_12 and feature_15 are the least important variables and feature_4, feature_12 are also less important variables in the model.
* feature_3 is the most important variable. Feature_11 and feature_13 are also important variables followed by feature_5 and feature_6.

# Marketting Suggestions:

* On Marketing perspective, business should be focused on feature_3 mostly. Because the increase of the value of feature_3 has the highest impact on customer who will churn . So business should identify the starategy to decrease the feature_3 value in future.

* Second focus should be on feature_5. Beacuse it was strong connections with feature_15. So business can identify a cost effective way to reduce the impact of feature_5: either reducing the value of feature_5 itself or increasing the value of feature_15. Whichever costs less is choosable.

* So in identifying who will churn, feature_3 is the most important factor. Then movement of feature_5, feature_11 and feature_13 should be monitored closely. Higher value of feature_11 and feature_13 is expected. So business should try to find the customer for which the feature_11 value is 1 and feature_13 value is 2. That will result getting a long term customer.


# Conclusion:

* The selected model is a multinomial logistic model. The model selection was mainly based on two factors. One is predicting churn (precision). Another one is computational cost as the other model KNN is costly (22 times) in terms of time for running the model. Though model showed same accuracy, but churn prediction was better by logistic model at the cost of higher mis-classification of the customers who will not churn.

* EDA and Shapley Value clearly indicates that feature_3 is the most important variable.

* As the target variable is highly imbalanced, the model characteristics is influenced significantly. 

* Inter correlation among the variables also caused negative effect in determining the model predictions.Moreover, the data distribution of different variables are not separatable for two classes of the labels which ultimately restricts to use linear regression model. 

* For improving the model performance, having more data related to churn will be useful as it will eliminate the nature of imbalanced target variable.


# References:

* Monash University  Monash Moodle, FIT 5149 Applied Data Analysi, viewed on 13 Nov 2021. https://lms.monash.edu/course/view.php?id=120537&section=5

* StackExchange , viewed on 13 Nov 2021, https://stats.stackexchange.com/questions/318968/knn-and-k-folding-in-r

* Stack Overflow
Stack Overflow - Where Developers Learn, Share, & Build Careers

